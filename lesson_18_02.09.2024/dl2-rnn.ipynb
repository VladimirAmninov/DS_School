{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Loading and Preprocessing Data\n\nFirst, we need to load and preprocess the data. For this example, we'll use the IMDB dataset, which consists of movie reviews labeled as positive or negative.\n\nWe'll preprocess the data by tokenizing the text and padding the sequences to ensure they have the same length.\n","metadata":{}},{"cell_type":"code","source":"# Loading and Preprocessing Data\n\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Load the IMDB dataset, keeping only the top 10,000 most frequently occurring words\nvocab_size = 10000\nmax_length = 200\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n\n# Pad the sequences to ensure they have the same length\nx_train = pad_sequences(x_train, maxlen=max_length)\nx_test = pad_sequences(x_test, maxlen=max_length)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:34:00.565596Z","iopub.execute_input":"2024-09-02T12:34:00.565957Z","iopub.status.idle":"2024-09-02T12:34:21.682654Z","shell.execute_reply.started":"2024-09-02T12:34:00.565927Z","shell.execute_reply":"2024-09-02T12:34:21.681341Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-09-02 12:34:02.787815: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-02 12:34:02.787960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-02 12:34:02.943692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Building the RNN Model\n\nNext, we'll build a simple RNN model using TensorFlow and Keras. The model consists of:\n1. **Embedding Layer:** Converts the input sequences into dense vectors of fixed size.\n2. **Recurrent Layer (LSTM):** Processes the input sequences and captures long-term dependencies.\n3. **Dense Layer:** Fully connected layer with 1 neuron and sigmoid activation for binary classification.\n","metadata":{}},{"cell_type":"code","source":"# Building the RNN Model\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN\n\n# Initialize a Sequential model\nmodel = Sequential()\n\n# Add an embedding layer to convert input sequences into dense vectors of fixed size\nmodel.add(Embedding(input_dim=vocab_size, output_dim=128))\n\n# Add an LSTM layer with 128 units to capture long-term dependencies\nmodel.add(SimpleRNN(128))\n\n# Add a fully connected layer with 1 neuron and sigmoid activation for binary classification\nmodel.add(Dense(1, activation='sigmoid'))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:35:31.275115Z","iopub.execute_input":"2024-09-02T12:35:31.275851Z","iopub.status.idle":"2024-09-02T12:35:31.317171Z","shell.execute_reply.started":"2024-09-02T12:35:31.275814Z","shell.execute_reply":"2024-09-02T12:35:31.315989Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Compiling the Model\n\nWe need to compile the model by specifying the optimizer, loss function, and metrics. We'll use the Adam optimizer, binary crossentropy loss function, and accuracy as the evaluation metric.\n\n- **Optimizer (Adam):** Efficient for training deep learning models.\n- **Loss Function (Binary Crossentropy):** Suitable for binary classification tasks.\n- **Metrics (Accuracy):** Evaluates the model's performance by calculating the percentage of correctly predicted instances.\n","metadata":{}},{"cell_type":"code","source":"# Compiling the Model\n\n# Compile the model by specifying the optimizer, loss function, and metrics\nmodel.compile(optimizer='adam',                        # Adam optimizer\n              loss='binary_crossentropy',              # Binary crossentropy loss function for binary classification\n              metrics=['accuracy'])                    # Evaluation metric: accuracy\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:35:35.244993Z","iopub.execute_input":"2024-09-02T12:35:35.245427Z","iopub.status.idle":"2024-09-02T12:35:35.266544Z","shell.execute_reply.started":"2024-09-02T12:35:35.245394Z","shell.execute_reply":"2024-09-02T12:35:35.265616Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Training the Model\n\nNow, we'll train the model using the training data. We'll set the number of epochs to 5 and use 20% of the training data for validation.\n\nAn epoch is one complete iteration over the entire training data. Validation data is used to evaluate the model's performance on data it hasn't seen during training, helping to detect overfitting.\n","metadata":{}},{"cell_type":"code","source":"# Training the Model\n\n# Train the model with the training data\nhistory = model.fit(x_train,                           # Training data\n                    y_train,                           # Training labels\n                    epochs=3,                          # Number of epochs\n                    validation_split=0.2)              # Use 20% of training data for validation\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:39:04.736931Z","iopub.execute_input":"2024-09-02T12:39:04.737346Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/3\n\u001b[1m413/625\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 46ms/step - accuracy: 0.8082 - loss: 0.4251","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluating the Model\n\nAfter training, we can evaluate the model's performance using the test data. We'll measure the test accuracy to see how well the model generalizes to new data.\n","metadata":{}},{"cell_type":"code","source":"# Evaluating the Model\n\n# Evaluate the model's performance using the test data\ntest_loss, test_acc = model.evaluate(x_test,           # Test data\n                                     y_test,           # Test labels\n                                     verbose=2)        # Verbose output for evaluation\nprint('\\nTest accuracy:', test_acc)                    # Print the test accuracy\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing Training Results\n\nLet's plot the training and validation accuracy and loss over the epochs to see how the model's performance improved during training.\n\nThese plots help in understanding the model's learning process and identifying potential issues like overfitting.\n","metadata":{}},{"cell_type":"code","source":"# Visualizing Training Results\n\nimport matplotlib.pyplot as plt\n\n# Plot training & validation accuracy values\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)                                   # Create subplot for accuracy\nplt.plot(history.history['accuracy'])                  # Plot training accuracy\nplt.plot(history.history['val_accuracy'])              # Plot validation accuracy\nplt.title('Model Accuracy')                            # Title of the plot\nplt.ylabel('Accuracy')                                 # Y-axis label\nplt.xlabel('Epoch')                                    # X-axis label\nplt.legend(['Train', 'Validation'], loc='upper left')  # Legend\n\n# Plot training & validation loss values\nplt.subplot(1, 2, 2)                                   # Create subplot for loss\nplt.plot(history.history['loss'])                      # Plot training loss\nplt.plot(history.history['val_loss'])                  # Plot validation loss\nplt.title('Model Loss')                                # Title of the plot\nplt.ylabel('Loss')                                     # Y-axis label\nplt.xlabel('Epoch')                                    # X-axis label\nplt.legend(['Train', 'Validation'], loc='upper left')  # Legend\n\nplt.show()                                             # Display the plots\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}