{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](img/homework.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Задание 2: '\\w{3}\\W\\w{10}\\W\\w{3}'\n",
        "https://www.hackerrank.com/challenges/matching-word-non-word/problem?isFullScreen=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Задание 1: \n",
        "Подсчитайте количество разных слов до и после лемматизации в рамках датасета с классной работы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xJfkstKpqsXp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/vladimir/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/vladimir/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import umap\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"What TV shows or books help you read people's body language?\\n\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
        "data[50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "print(tokenizer.tokenize(data[50]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['what', 'are', 'some', 'ways', 'to', 'overcome', 'a', 'fast', 'food', 'addiction', '?']\n"
          ]
        }
      ],
      "source": [
        "data_tok = []\n",
        "\n",
        "for x in data:\n",
        "    data_tok.append(tokenizer.tokenize(x.lower()))\n",
        "\n",
        "print(data_tok[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#checking\n",
        "\n",
        "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
        "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
        "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество строк до стемминга и лемматизации: 535\n"
          ]
        }
      ],
      "source": [
        "dictionary_before_stem_lem = {}\n",
        "for i in range(100):\n",
        "    for j in range(len(data_tok[i])):\n",
        "        if data_tok[i][j] in dictionary_before_stem_lem.keys():\n",
        "            dictionary_before_stem_lem[data_tok[i][j]] = dictionary_before_stem_lem[data_tok[i][j]] + 1\n",
        "        else:\n",
        "            dictionary_before_stem_lem[data_tok[i][j]] = 1\n",
        "\n",
        "df_before_stem_lem = pd.DataFrame([dictionary_before_stem_lem]).T\n",
        "\n",
        "print(f'Количество строк до стемминга и лемматизации: {len(df_before_stem_lem)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество строк до лемматизации: 513\n"
          ]
        }
      ],
      "source": [
        "dictionary_before_lem = {}\n",
        "for i in range(100):\n",
        "    for j in range(len(data_tok[i])):\n",
        "      stem = ps.stem(data_tok[i][j])\n",
        "      if stem in dictionary_before_lem.keys():\n",
        "          dictionary_before_lem[stem] = dictionary_before_lem[stem] + 1\n",
        "      else: \n",
        "        dictionary_before_lem[stem] = 1\n",
        "\n",
        "df_before_lem = pd.DataFrame([dictionary_before_lem]).T\n",
        "\n",
        "print(f'Количество строк до лемматизации: {len(df_before_lem)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество строк после лемматизации: 512\n"
          ]
        }
      ],
      "source": [
        "dictionary_after_lem = {}\n",
        "for i in range(100):\n",
        "    for j in range(len(data_tok[i])):\n",
        "      stem = ps.stem(data_tok[i][j])\n",
        "      stem = lemmatizer.lemmatize(stem)\n",
        "      if stem in dictionary_after_lem.keys():\n",
        "         dictionary_after_lem[stem] = dictionary_after_lem[stem] + 1\n",
        "      else:\n",
        "         dictionary_after_lem[stem] = 1\n",
        "\n",
        "df_after_lem = pd.DataFrame([dictionary_after_lem]).T\n",
        "\n",
        "print(f'Количество строк после лемматизации: {len(df_after_lem)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Итого: \n",
        "1. всего уникальных слов в первой сотне предложений data_tok составляет 535\n",
        "2. после стемминга - 513\n",
        "3. после лемматизации - 512"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLP1_homework",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
